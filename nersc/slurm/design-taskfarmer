#!/bin/bash
#SBATCH --qos regular
#SBATCH --constraint haswell
#SBATCH --nodes 200
#SBATCH --cpus-per-task 64
#SBATCH --time 24:00:00
#SBATCH --license cscratch1
#SBATCH --mail-type ALL
#SBATCH --mail-user morelandjs@gmail.com
#SBATCH --output  /global/cscratch1/sd/jsm55/slurm/%x-%j
#SBATCH --error   /global/cscratch1/sd/jsm55/slurm/%x-%j
#SBATCH --workdir /global/cscratch1/sd/jsm55/inputfiles


# export environment variables
export CONDA_PREFIX=/global/common/software/m2730/qm18-partons
export PATH=$CONDA_PREFIX/bin:$PATH
export XDG_DATA_HOME=$CSCRATCH/share
export THREADS=32

# design input files
design=qm18-partons/main/pPb5020
inputfiles=$(find $design -type f)
ntasks=200

# unique job identifier
job=$NERSC_HOST-$SLURM_JOB_ID

# list of input files and grid sizes
function design_point_gridsize {
  for file in $inputfiles; do
    width=$(grep parton-width $file | awk '{print $NF}')
    echo $width $file
  done
}

# sort input files by grid size (proxy for run time)
inputfiles=$(design_point_gridsize | sort | awk '{print $NF}')

# create taskfarmer working directory
workdir=$CSCRATCH/taskfarmer/$SLURM_JOB_NAME-$NERSC_HOST-$SLURM_JOB_ID
mkdir -p $workdir
cd $workdir

# create taskfarmer tasks
for file in $inputfiles; do
  for task in $(seq -f "%05g" $ntasks); do
    echo run-events \
      --nevents 25 \
      --grid-scale 0.2 \
      --logfile $CSCRATCH/logs/$job/$file/$task.log \
      @$CSCRATCH/inputfiles/$file \
      $CSCRATCH/events/$file/$job/$task.dat >> tasks
  done
done

# run tasks
module load taskfarmer
runcommands.sh tasks
